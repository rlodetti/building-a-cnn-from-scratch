{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a CNN from Scratch - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Now that you have background knowledge regarding how Convolution Neural Networks (CNNs) work and how to build them using Keras, its time to practice those skills a little more independently in order to build a CNN (or ConvNet) on your own to solve a image recognition problem. In this lab, you'll practice building an image classifier from start to finish using a CNN.  \n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab you will: \n",
    "\n",
    "- Load images from a hierarchical file structure using an image datagenerator \n",
    "- Apply data augmentation to image files before training a neural network \n",
    "- Build a CNN using Keras \n",
    "- Visualize and evaluate the performance of CNN models \n",
    "\n",
    "## Loading the Images\n",
    "\n",
    "The data for this lab are a bunch of pictures of cats and dogs, and our task is to correctly classify a picture as one or the other. The [original dataset](https://www.kaggle.com/c/dogs-vs-cats) is from Kaggle. We have downsampled this dataset in order to reduce training time for you when you design and fit your model to the data. ⏰ It is anticipated that this process will take approximately one hour to run on a standard machine, although times will vary depending on your particular computer and set up. At the end of this lab, you are welcome to try training on the complete dataset and observe the impact on the model's overall accuracy. \n",
    "\n",
    "You can find the initial downsampled dataset in a subdirectory, **cats_dogs_downsampled**, of this repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images\n",
    "\n",
    "train_dir = 'cats_dogs_downsampled/train'\n",
    "validation_dir = 'cats_dogs_downsampled/val/'\n",
    "test_dir = 'cats_dogs_downsampled/test/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up date time to track how long run time takes\n",
    "import datetime\n",
    "\n",
    "original_start = datetime.datetime.now()\n",
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 08:21:16.244329: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2140 images belonging to 2 classes.\n",
      "Found 420 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the images into tensors\n",
    "# Rescale the data by 1/.255 and use binary_crossentropy loss\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(validation_dir,\n",
    "                                                        target_size=(150, 150),\n",
    "                                                        batch_size=20,\n",
    "                                                        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the Model\n",
    "\n",
    "Now it's time to design your CNN using Keras. Remember a few things when doing this: \n",
    "\n",
    "- You should alternate convolutional and pooling layers\n",
    "- You should have later layers have a larger number of parameters in order to detect more abstract patterns\n",
    "- Add some final dense layers to add a classifier to the convolutional base \n",
    "- Compile this model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design the model\n",
    "# Note: You may get a comment from tf regarding your kernel. This is not a warning per se, but rather informational.\n",
    "import numpy as np\n",
    "from tensorflow.keras import models, layers\n",
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150 ,150,  3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating the Model\n",
    "\n",
    "Remember that training deep networks is resource intensive: depending on the size of the data, even a CNN with 3-4 successive convolutional and pooling layers is apt to take a hours to train on a high end laptop. See the code chunk below to see how long it took to run your model. \n",
    "\n",
    "If you are concerned with runtime, you may want to set your model to run the training epochs overnight.  \n",
    "\n",
    "**If you are going to run this process overnight, be sure to also script code for the following questions concerning data augmentation. Check your code twice (or more) and then set the notebook to run all, or something equivalent to have them train overnight.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 28s 273ms/step - loss: 0.6907 - acc: 0.5285 - val_loss: 0.6899 - val_acc: 0.5850\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 27s 271ms/step - loss: 0.6880 - acc: 0.5360 - val_loss: 0.6886 - val_acc: 0.5300\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 28s 275ms/step - loss: 0.6860 - acc: 0.5465 - val_loss: 0.6852 - val_acc: 0.6125\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 27s 270ms/step - loss: 0.6835 - acc: 0.5530 - val_loss: 0.6836 - val_acc: 0.5350\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 28s 276ms/step - loss: 0.6800 - acc: 0.5620 - val_loss: 0.6843 - val_acc: 0.5225\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 27s 274ms/step - loss: 0.6777 - acc: 0.5820 - val_loss: 0.6803 - val_acc: 0.5350\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 28s 275ms/step - loss: 0.6748 - acc: 0.5845 - val_loss: 0.6734 - val_acc: 0.5900\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 28s 277ms/step - loss: 0.6708 - acc: 0.5940 - val_loss: 0.6777 - val_acc: 0.5600\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 28s 277ms/step - loss: 0.6680 - acc: 0.5940 - val_loss: 0.6862 - val_acc: 0.5250\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 27s 274ms/step - loss: 0.6645 - acc: 0.6115 - val_loss: 0.6632 - val_acc: 0.6025\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 0.6562 - acc: 0.6160 - val_loss: 0.6877 - val_acc: 0.5350\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 28s 281ms/step - loss: 0.6512 - acc: 0.6260 - val_loss: 0.6542 - val_acc: 0.6125\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 27s 274ms/step - loss: 0.6521 - acc: 0.6360 - val_loss: 0.6718 - val_acc: 0.5775\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 27s 275ms/step - loss: 0.6466 - acc: 0.6315 - val_loss: 0.6940 - val_acc: 0.5275\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 27s 274ms/step - loss: 0.6443 - acc: 0.6375 - val_loss: 0.6509 - val_acc: 0.6225\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 27s 274ms/step - loss: 0.6323 - acc: 0.6455 - val_loss: 0.6581 - val_acc: 0.6025\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 28s 275ms/step - loss: 0.6268 - acc: 0.6570 - val_loss: 0.6422 - val_acc: 0.6425\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 28s 275ms/step - loss: 0.6211 - acc: 0.6660 - val_loss: 0.6413 - val_acc: 0.6350\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 28s 278ms/step - loss: 0.6107 - acc: 0.6610 - val_loss: 0.6442 - val_acc: 0.6475\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 28s 278ms/step - loss: 0.6105 - acc: 0.6680 - val_loss: 0.7531 - val_acc: 0.5550\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 27s 274ms/step - loss: 0.5951 - acc: 0.6940 - val_loss: 0.6385 - val_acc: 0.6500\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 27s 273ms/step - loss: 0.5846 - acc: 0.6935 - val_loss: 0.6334 - val_acc: 0.6175\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 28s 278ms/step - loss: 0.5944 - acc: 0.6750 - val_loss: 0.6236 - val_acc: 0.6150\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.5874 - acc: 0.6900 - val_loss: 0.6209 - val_acc: 0.6400\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 28s 278ms/step - loss: 0.5707 - acc: 0.6975 - val_loss: 0.6298 - val_acc: 0.6425\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 28s 276ms/step - loss: 0.5608 - acc: 0.7055 - val_loss: 0.6193 - val_acc: 0.6400\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 28s 280ms/step - loss: 0.5510 - acc: 0.7130 - val_loss: 0.6267 - val_acc: 0.6500\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 28s 278ms/step - loss: 0.5367 - acc: 0.7345 - val_loss: 0.5968 - val_acc: 0.6800\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 33s 327ms/step - loss: 0.5256 - acc: 0.7350 - val_loss: 0.5809 - val_acc: 0.7000\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 31s 307ms/step - loss: 0.5161 - acc: 0.7470 - val_loss: 0.6198 - val_acc: 0.6675\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'builtin_function_or_method' and 'datetime.datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_generator, \n\u001b[1;32m      7\u001b[0m                               steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \n\u001b[1;32m      8\u001b[0m                               epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, \n\u001b[1;32m      9\u001b[0m                               validation_data\u001b[38;5;241m=\u001b[39mvalidation_generator, \n\u001b[1;32m     10\u001b[0m                               validation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     11\u001b[0m end \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow\n\u001b[0;32m---> 12\u001b[0m duration \u001b[38;5;241m=\u001b[39m \u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Print duration\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuration:\u001b[39m\u001b[38;5;124m\"\u001b[39m, duration)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'builtin_function_or_method' and 'datetime.datetime'"
     ]
    }
   ],
   "source": [
    "# Set the model to train \n",
    "# Note: You may get a comment from tf regarding your GPU or sometning similar.\n",
    "# This is not a warning per se, but rather informational.\n",
    "# ⏰ This cell may take several minutes to run \n",
    "start = datetime.datetime.now()\n",
    "history = model.fit(train_generator, \n",
    "                              steps_per_epoch=100, \n",
    "                              epochs=30, \n",
    "                              validation_data=validation_generator, \n",
    "                              validation_steps=20)\n",
    "end = datetime.datetime.now()\n",
    "duration = end - start\n",
    "\n",
    "# Print duration\n",
    "print(\"Duration:\", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def visualize_training_results(results):\n",
    "    history = results.history\n",
    "    plt.figure()\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.plot(history['loss'])\n",
    "    plt.legend(['val_loss', 'loss'])\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(history['val_acc'])\n",
    "    plt.plot(history['acc'])\n",
    "    plt.legend(['val_acc', 'acc'])\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "# Type code here for plot history\n",
    "visualize_training_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check runtime\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print('Training took a total of {}'.format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for future reference \n",
    "model.save('baseline_downsampled_data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Recall that data augmentation is typically always a necessary step when using a small dataset as this one which you have been provided. If you haven't already, implement a data augmentation setup.\n",
    "\n",
    "**Warning: ⏰ This process may take awhile depending on your set-up. As such, make allowances for this as necessary.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up date time to track how long run time takes\n",
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data augmentation to the model setup and set the model to train; \n",
    "# See the warnings above if you intend to run these blocks of code \n",
    "# ⏰ These cells where may take quite some time to run\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=40, \n",
    "                                   width_shift_range=0.2, \n",
    "                                   height_shift_range=0.2, \n",
    "                                   shear_range=0.2, \n",
    "                                   zoom_range=0.2, \n",
    "                                   horizontal_flip=True, \n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "history_2 = model.fit(train_generator, \n",
    "                              steps_per_epoch=100, \n",
    "                              epochs=30, \n",
    "                              validation_data=validation_generator, \n",
    "                              validation_steps=20)\n",
    "visualize_training_results(history_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check runtime \n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print('Training took a total of {}'.format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model for future reference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model \n",
    "model.save('augmented_downsampled_data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation\n",
    "\n",
    "Now use the test set to perform a final evaluation on your model of choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a final evaluation using the test set\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, \n",
    "                                                  target_size=(150, 150), \n",
    "                                                  batch_size=20, \n",
    "                                                  class_mode='binary')\n",
    "test_loss, test_acc = model.evaluate(test_generator, steps=20)\n",
    "print('test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Well done. In this lab, you practice building your own CNN for image recognition which drastically outperformed our previous attempts using a standard deep learning model alone. In the upcoming sections, we'll continue to investigate further techniques associated with CNNs including visualizing the representations they learn and techniques to further bolster their performance when we have limited training data such as here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
